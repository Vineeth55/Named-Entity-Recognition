{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "from numpy import argmax\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext.data as data\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe, Vocab\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.metrics import f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tagged sentences: 8936\n",
      "Vocabulary size: 17259\n",
      "Total number of tags: 22\n"
     ]
    }
   ],
   "source": [
    "# def read_file(filename):\n",
    "#     with open(filename, \"r\") as file:\n",
    "#         text = file.readlines()\n",
    "#     return text\n",
    "\n",
    "# def process_text(text):\n",
    "#     X = []\n",
    "#     Y = []\n",
    "#     sentenceX = []\n",
    "#     sentenceY = []\n",
    "#     for line in text:\n",
    "#         split = line.split(\" \")\n",
    "#         if len(split) > 1:\n",
    "#             sentenceX.append(split[0])\n",
    "#             sentenceY.append(split[1].replace(\"\\n\", \"\"))\n",
    "#         else:\n",
    "#             X.append(sentenceX)\n",
    "#             Y.append(sentenceY)\n",
    "#             sentenceX = []\n",
    "#             sentenceY = []\n",
    "#     return X, Y\n",
    "\n",
    "# train_data = read_file(\"data/train.txt\")\n",
    "# test_data = read_file(\"data/test.txt\")\n",
    "# X1, Y1 = process_text(train_data)\n",
    "# X2, Y2 = process_text(test_data)\n",
    "\n",
    "# vocab = set([word.lower() for sentence in X1 for word in sentence])\n",
    "# vocab.add(\"unk\")\n",
    "# vocab = list(vocab)\n",
    "# num_words = len(vocab)\n",
    "# num_tags = len(set([word.lower() for sentence in Y1 for word in sentence]))\n",
    "\n",
    "# print(\"Total number of tagged sentences: {}\".format(len(X1)))\n",
    "# print(\"Vocabulary size: {}\".format(num_words))\n",
    "# print(\"Total number of tags: {}\".format(num_tags))\n",
    "\n",
    "# # text_field = data.Field(sequential=True)\n",
    "# # dataset = data.Dataset(examples=vocab, fields=[('text', text_field), ...])\n",
    "# # text_field.build_vocab(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chancellor O\n",
      "of B-PP\n",
      "the B-NP\n",
      "Exchequer I-NP\n",
      "Nigel B-NP\n",
      "Lawson I-NP\n",
      "'s B-NP\n",
      "restated I-NP\n",
      "commitment I-NP\n",
      "to B-PP\n",
      "a B-NP\n",
      "firm I-NP\n",
      "monetary I-NP\n",
      "policy I-NP\n",
      "has B-VP\n",
      "helped I-VP\n",
      "to I-VP\n",
      "prevent I-VP\n",
      "a B-NP\n",
      "freefall I-NP\n",
      "in B-PP\n",
      "sterling B-NP\n",
      "over B-PP\n",
      "the B-NP\n",
      "past I-NP\n",
      "week I-NP\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(X1[1])):\n",
    "#     print(X1[1][i], Y1[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encoding(sentence, to_idx):\n",
    "#     idxs = torch.tensor([to_idx[w.lower()] for w in sentence], dtype=torch.long)\n",
    "#     # for w in sentence:\n",
    "#     #     try:\n",
    "#     #         idxs.append(to_idx[w.lower])\n",
    "#     #     except KeyError:\n",
    "#     #         continue\n",
    "#     # idxs = torch.tensor(idxs, dtype=torch.long)\n",
    "#     return idxs\n",
    "\n",
    "\n",
    "# word_to_idx = {}\n",
    "# tag_to_idx = {}\n",
    "# for i in range(len(X1)):\n",
    "#     sentence = X1[i]\n",
    "#     tags = Y1[i]\n",
    "\n",
    "#     for word in sentence:\n",
    "#         if word.lower() not in word_to_idx:\n",
    "#             word_to_idx[word.lower()] = len(word_to_idx)\n",
    "    \n",
    "#     word_to_idx[\"unk\"] = len(word_to_idx)\n",
    "\n",
    "#     for tag in tags:\n",
    "#         if tag.lower() not in tag_to_idx:\n",
    "#             tag_to_idx[tag.lower()] = len(tag_to_idx)\n",
    "# # print(word_to_idx)\n",
    "# EMBEDDING_DIM = 100\n",
    "# HIDDEN_DIM = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMTagger(nn.Module):\n",
    "    \n",
    "#     def __init__(self, embedding_dim, hidden_dim, num_words, num_tags):\n",
    "#         super(LSTMTagger, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.vocab = vocab\n",
    "\n",
    "#         self.embeddings = nn.Embedding(num_words, embedding_dim)\n",
    "\n",
    "#         # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "#         # with dimensionality hidden_dim.\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "#         # The linear layer that maps from hidden state space to tag space\n",
    "#         self.hidden2tag = nn.Linear(hidden_dim, num_tags)\n",
    "#     # def _init_embeddings(self):\n",
    "#     #     glove = GloVe(name=\"6B\", dim=300, cache=\"E:\\IIT DELHI\\SEMESTER-8\\ELL881\\ASSIGNMENTS\\A2\\.vector_cache\")\n",
    "#     #     # TODO not initialize with zero for non existent vocab items in glove\n",
    "#     #     self.embeddings.weight.data.copy_(glove.get_vecs_by_tokens(self.vocab.get_itos(), lower_case_backup=True))\n",
    "        \n",
    "#     def forward(self, sentence):\n",
    "#         embeds = self.embeddings(sentence)\n",
    "#         # print(embeds.shape)\n",
    "#         lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "#         tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "#         tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "#         return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EMBEDDING_DIM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\IIT DELHI\\SEMESTER-8\\ELL881\\ASSIGNMENTS\\A2\\a2.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/IIT%20DELHI/SEMESTER-8/ELL881/ASSIGNMENTS/A2/a2.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, \u001b[39mlen\u001b[39m(word_to_idx), \u001b[39mlen\u001b[39m(tag_to_idx))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/IIT%20DELHI/SEMESTER-8/ELL881/ASSIGNMENTS/A2/a2.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loss_function \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mNLLLoss()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/IIT%20DELHI/SEMESTER-8/ELL881/ASSIGNMENTS/A2/a2.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EMBEDDING_DIM' is not defined"
     ]
    }
   ],
   "source": [
    "# model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_idx), len(tag_to_idx))\n",
    "# loss_function = nn.NLLLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# # See what the scores are before training\n",
    "# # Note that element i,j of the output is the score for tag j for word i.\n",
    "# # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "# # with torch.no_grad():\n",
    "# #     inputs = encoding(X1[0], word_to_idx)\n",
    "# #     tag_scores = model(inputs)\n",
    "# #     print(tag_scores)\n",
    "\n",
    "# for epoch in range(100):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "#     # print(epoch)\n",
    "#     for i in range(2):\n",
    "#         sentence = X1[i]\n",
    "#         tags = Y1[i]\n",
    "#         # Step 1. Remember that Pytorch accumulates gradients.\n",
    "#         # We need to clear them out before each instance\n",
    "#         model.zero_grad()\n",
    "\n",
    "#         # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "#         # Tensors of word indices.\n",
    "#         targets = encoding(tags, tag_to_idx)\n",
    "#         # print(targets)\n",
    "#         sentence_in = encoding(sentence, word_to_idx)\n",
    "#         # print(sentence_in)\n",
    "#         targets = encoding(tags, tag_to_idx)\n",
    "\n",
    "#         # Step 3. Run our forward pass.\n",
    "#         tag_scores = model(sentence_in)\n",
    "\n",
    "#         # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "#         #  calling optimizer.step()\n",
    "#         loss = loss_function(tag_scores, targets)\n",
    "#         print(loss)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # See what the scores are after training\n",
    "# with torch.no_grad():\n",
    "#     inputs = encoding(X1[0], word_to_idx)\n",
    "#     tag_scores = model(inputs)\n",
    "\n",
    "#     # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#     # for word i. The predicted tag is the maximum scoring tag.\n",
    "#     # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "#     # since 0 is index of the maximum value of row 1,\n",
    "#     # 1 is the index of maximum value of row 2, etc.\n",
    "#     # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "#     print(tag_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_scores(predY, trueY):\n",
    "\n",
    "#     trueY_O = [i for i, x in enumerate(trueY) if x == \"O\"]\n",
    "#     predY = [predY[i] for i in range(len(predY)) if i not in trueY_O]\n",
    "#     trueY = [trueY[i] for i in range(len(trueY)) if i not in trueY_O]\n",
    "\n",
    "#     print(\"Micro F1 score: \", f1_score(trueY, predY, average=\"micro\"))\n",
    "#     print(\"Macro F1 score: \", f1_score(trueY, predY, average=\"macro\"))\n",
    "#     print(\"Average F1 score: \", (f1_score(trueY, predY, average=\"micro\") + f1_score(trueY, predY, average=\"macro\")) / 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PP', 'B-NP', 'I-NP', 'B-NP', 'I-NP', 'B-NP', 'I-NP', 'I-NP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-VP', 'I-VP', 'I-VP', 'I-VP', 'B-NP', 'I-NP', 'B-PP', 'B-NP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'O']\n",
      "['O', 'B-PP', 'B-NP', 'I-NP', 'B-NP', 'I-NP', 'B-NP', 'I-NP', 'I-NP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'I-VP', 'I-VP', 'I-VP', 'I-VP', 'B-NP', 'I-NP', 'B-PP', 'B-NP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'O']\n",
      "Micro F1 score:  0.96\n",
      "Macro F1 score:  0.7714285714285715\n",
      "Average F1 score:  0.8657142857142857\n"
     ]
    }
   ],
   "source": [
    "# #Testing the model\n",
    "# tag_keys = list(tag_to_idx.keys())\n",
    "# tag_values = list(tag_to_idx.values())\n",
    "# sentence_in = X1[1]\n",
    "# true_Y = Y1[1]\n",
    "# with torch.no_grad():\n",
    "#     inputs = encoding(sentence_in, word_to_idx)\n",
    "#     tag_scores = model(inputs)\n",
    "# pred_Y = [tag_keys[position].upper() for position in [tag_values.index(argmax(y)) for y in tag_scores]]\n",
    "# print(true_Y)\n",
    "# print(pred_Y)\n",
    "\n",
    "\n",
    "\n",
    "# get_scores(pred_Y, true_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11555,     8,     2,  1774,    16,  1163,   176,     5,   211,   316,\n",
      "         1215,  6115,   107,   649,   596,    10,   456,     1,   248,    10,\n",
      "         2625,  2962,     1,  4919,     5,   601,     6,  1400,  1751,    20,\n",
      "          731,     7,   542,     9,  8901, 14950,     3])\n",
      "['Confidence', 'in', 'the', 'pound', 'is', 'widely', 'expected', 'to', 'take', 'another', 'sharp', 'dive', 'if', 'trade', 'figures', 'for', 'September', ',', 'due', 'for', 'release', 'tomorrow', ',', 'fail', 'to', 'show', 'a', 'substantial', 'improvement', 'from', 'July', 'and', 'August', \"'s\", 'near-record', 'deficits', '.']\n",
      "['B-NP', 'B-PP', 'B-NP', 'I-NP', 'B-VP', 'I-VP', 'I-VP', 'I-VP', 'I-VP', 'B-NP', 'I-NP', 'I-NP', 'B-SBAR', 'B-NP', 'I-NP', 'B-PP', 'B-NP', 'O', 'B-ADJP', 'B-PP', 'B-NP', 'B-NP', 'O', 'B-VP', 'I-VP', 'I-VP', 'B-NP', 'I-NP', 'I-NP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'B-NP', 'I-NP', 'I-NP', 'O']\n",
      "tensor([1, 4, 1, 0, 3, 5, 5, 5, 5, 1, 0, 0, 7, 1, 0, 4, 1, 2, 8, 4, 1, 1, 2, 3,\n",
      "        5, 5, 1, 0, 0, 4, 1, 0, 0, 1, 0, 0, 2])\n",
      "Total number of tagged sentences: 8936\n",
      "Vocabulary size: 19123\n",
      "Total number of tags: 22\n"
     ]
    }
   ],
   "source": [
    "class ELL881Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, vocab=None, data_dir=None, file_path=None, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.split=split\n",
    "\n",
    "        if file_path is not None:\n",
    "            self.file_path = file_path\n",
    "        else:\n",
    "            self.file_path = os.path.join(data_dir, f\"{split}.txt\")\n",
    "        self.text = self.read_file(self.file_path)\n",
    "        self.X, self.Y = self.process_text(self.text)\n",
    "        self.vocab = build_vocab_from_iterator(self.X, specials=[\"<unk>\"], min_freq=1)\n",
    "        self.tags = build_vocab_from_iterator(self.Y, min_freq=1)\n",
    "        self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def read_file(self, filename):\n",
    "        with open(filename, \"r\") as file:\n",
    "            text = file.readlines()\n",
    "        return text\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        X = []\n",
    "        Y = []\n",
    "        sentenceX = []\n",
    "        sentenceY = []\n",
    "        for line in text:\n",
    "            split = line.split(\" \")\n",
    "            if len(split) > 1:\n",
    "                sentenceX.append(split[0])\n",
    "                sentenceY.append(split[1].replace(\"\\n\", \"\"))\n",
    "            else:\n",
    "                X.append(sentenceX)\n",
    "                Y.append(sentenceY)\n",
    "                sentenceX = []\n",
    "                sentenceY = []\n",
    "        return X, Y\n",
    "    \n",
    "    def tokenize_sent(self, sentence):\n",
    "        return torch.tensor(self.vocab(sentence)).long()\n",
    "    \n",
    "    def tokenize_tags(self, targets):\n",
    "        return torch.tensor(self.tags(targets)).long()\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "train_dataset = ELL881Dataset(data_dir=\"E:\\IIT DELHI\\SEMESTER-8\\ELL881\\ASSIGNMENTS\\A2\\data\")\n",
    "test_dataset = ELL881Dataset(data_dir=\"E:\\IIT DELHI\\SEMESTER-8\\ELL881\\ASSIGNMENTS\\A2\\data\", split=\"test\")\n",
    "print(train_dataset.tokenize_sent(train_dataset.X[0]))\n",
    "print(train_dataset.X[0])\n",
    "print(train_dataset.Y[0])\n",
    "print(train_dataset.tokenize_tags(train_dataset.Y[0]))\n",
    "print(\"Total number of tagged sentences: {}\".format(len(train_dataset)))\n",
    "print(\"Vocabulary size: {}\".format(train_dataset.vocab.__len__()))\n",
    "print(\"Total number of tags: {}\".format(train_dataset.tags.__len__()))\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence B-NP\n",
      "in B-PP\n",
      "the B-NP\n",
      "pound I-NP\n",
      "is B-VP\n",
      "widely I-VP\n",
      "expected I-VP\n",
      "to I-VP\n",
      "take I-VP\n",
      "another B-NP\n",
      "sharp I-NP\n",
      "dive I-NP\n",
      "if B-SBAR\n",
      "trade B-NP\n",
      "figures I-NP\n",
      "for B-PP\n",
      "September B-NP\n",
      ", O\n",
      "due B-ADJP\n",
      "for B-PP\n",
      "release B-NP\n",
      "tomorrow B-NP\n",
      ", O\n",
      "fail B-VP\n",
      "to I-VP\n",
      "show I-VP\n",
      "a B-NP\n",
      "substantial I-NP\n",
      "improvement I-NP\n",
      "from B-PP\n",
      "July B-NP\n",
      "and I-NP\n",
      "August I-NP\n",
      "'s B-NP\n",
      "near-record I-NP\n",
      "deficits I-NP\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_dataset.X[0])):\n",
    "    print(train_dataset.X[0][i], train_dataset.Y[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size, vocab : Vocab) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab = vocab\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self._init_embeddings()\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        self.loss_func = nn.NLLLoss()\n",
    "\n",
    "    def _init_embeddings(self):\n",
    "        glove = GloVe(name=\"6B\",dim=self.embedding_dim,cache=\"E:\\IIT DELHI\\SEMESTER-8\\ELL881\\ASSIGNMENTS\\A2\\.vector_cache\",)\n",
    "        self.embeddings.weight.data.copy_(glove.get_vecs_by_tokens(self.vocab.get_itos(), lower_case_backup=True))\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "    \n",
    "    def forward(self, sentence, targets=None):\n",
    "        embedding = self.embeddings(sentence)\n",
    "        out, _ = self.lstm(embedding.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        if targets is not None:\n",
    "            loss = self.loss_func(tag_scores, targets)\n",
    "            return loss, tag_scores\n",
    "        return tag_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--eta ETA] [--epochs EPOCHS]\n",
      "                             [--do_train DO_TRAIN] [--do_predict DO_PREDICT]\n",
      "                             [--do_eval DO_EVAL] [--output_dir OUTPUT_DIR]\n",
      "                             [--pred_file_path PRED_FILE_PATH]\n",
      "                             [--data_dir DATA_DIR]\n",
      "                             [--embedding_size EMBEDDING_SIZE]\n",
      "                             [--hidden_dim HIDDEN_DIM]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"8166658a-429f-4f89-b0e9-7f32d4b8aa05\" --shell=9007 --transport=\"tcp\" --iopub=9009 --f=c:\\Users\\admin\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-7900Nlk7djXY0Utl.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# # @dataclass\n",
    "# # class Params:\n",
    "    \n",
    "# #     eval_steps: int = 100\n",
    "# #     save_steps: int = 100\n",
    "# #     logging_steps: int = 50\n",
    "# #     warmup_steps: int = 0\n",
    "# #     # learning_rate: float = 0.1\n",
    "# #     lr_scheduler_type: str = \"linear\"\n",
    "# #     sgd_epsilon: float = 1e-8\n",
    "# #     weight_decay: int = 0\n",
    "# #     save_total_limit: int = 5\n",
    "# #     eval_accumulation_steps: int = 5\n",
    "# #     gradient_accumulation_steps: int = 1\n",
    "# #     early_stopping_patience: int = 5\n",
    "# #     early_stopping_threshold: float = 0.001\n",
    "# #     train_batch_size: int = 32\n",
    "# #     eval_batch_size: int = 32\n",
    "# #     prediction_loss_only: bool = False\n",
    "\n",
    "\n",
    "# #     # output_dir: str = \"models/\"\n",
    "# #     # do_train: bool = False\n",
    "# #     # do_eval: bool = True\n",
    "# #     # do_predict: bool = False\n",
    "# #     # prediction_file_path: str = \"data/test.txt\"\n",
    "# #     resume_from_checkpoint: bool = False\n",
    "# #     # num_train_epochs: int = 10\n",
    "# #     run_name: str = \"test\"\n",
    "\n",
    "# #     model_init_dir: str = None\n",
    "# #     # data_dir: str = \"data\"\n",
    "# #     do_freeze: bool = True\n",
    "# #     force_same_lr: bool = False\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--eta\", default=0.1, type=float, help=\"Learning Rate\")\n",
    "# parser.add_argument(\"--epochs\", default=10, type=int, help=\"Number of training epochs\")\n",
    "# parser.add_argument(\"--do_train\", default=True, type=bool, help=\"Whether Training has to be performed or not\")\n",
    "# parser.add_argument(\"--do_predict\", default=False, type=bool, help=\"\")\n",
    "# parser.add_argument(\"--do_eval\", default=False, type=bool, help=\"\")\n",
    "# parser.add_argument(\"--output_dir\", default=\"models/\", type=str, help=\"\")\n",
    "# parser.add_argument(\"--pred_file_path\", default=\"data/test.txt\", type=str, help=\"\")\n",
    "# parser.add_argument(\"--data_dir\", default=\"data\", type=str, help=\"\")\n",
    "# parser.add_argument(\"--embedding_size\", default=300, type=int, help=\"\")\n",
    "# parser.add_argument(\"--hidden_dim\", default=128, type=int, help=\"\")\n",
    "# # parser.add_argument()\n",
    "# # parser.add_argument()\n",
    "# # parser.add_argument()\n",
    "# # parser.add_argument()\n",
    "# # parser.add_argument()\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_dataset, eval_dataset, optimizer) -> None:\n",
    "        # self.args = args\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = eval_dataset\n",
    "        self.optim = optimizer\n",
    "        # for p in self.model.parameters():\n",
    "        #     if p.requires_grad == True:\n",
    "        #         print(p.shape)     \n",
    "\n",
    "    def train(self):\n",
    "\n",
    "\n",
    "        for epoch in range(10):\n",
    "            print(epoch)\n",
    "            for i in range(len(train_dataset)):\n",
    "                sentence = self.train_dataset.X[i]\n",
    "                tags = self.train_dataset.Y[i]\n",
    "                \n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of word indices.\n",
    "                inputs = self.train_dataset.tokenize_sent(sentence)\n",
    "                targets = self.train_dataset.tokenize_tags(tags)\n",
    "\n",
    "                # Step 3. Run our forward pass and get loss values.\n",
    "                loss, tag_scores = self.model(inputs, targets)\n",
    "                # Step 4. Compute the gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss.backward()\n",
    "\n",
    "                nn.utils.clip_grad_value_(self.model.parameters(), clip_value=1.0)\n",
    "                optimizer.step()\n",
    "    \n",
    "    def predict(self, dataset : ELL881Dataset):\n",
    "        output = []\n",
    "        for i in range(len(dataset)):\n",
    "            sentence = dataset.X[i]\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            input = train_dataset.tokenize_sent(sentence)\n",
    "            tag_scores = self.model.forward(input)\n",
    "            pred = [train_dataset.tags.lookup_token(torch.argmax(y)) for y in tag_scores]\n",
    "            output += pred \n",
    "\n",
    "        return output\n",
    "\n",
    "            \n",
    "            \n",
    "    def evaluate(self, trueY, predY):\n",
    "        trueY_O = [i for i, x in enumerate(trueY) if x == \"O\"]\n",
    "        predY = [predY[i] for i in range(len(predY)) if i not in trueY_O]\n",
    "        trueY = [trueY[i] for i in range(len(trueY)) if i not in trueY_O]\n",
    "\n",
    "        print(\"Micro F1 score: \", f1_score(trueY, predY, average=\"micro\"))\n",
    "        print(\"Macro F1 score: \", f1_score(trueY, predY, average=\"macro\"))\n",
    "        print(\"Average F1 score: \", (f1_score(trueY, predY, average=\"micro\") + f1_score(trueY, predY, average=\"macro\")) / 2)\n",
    "\n",
    "    # def save_model(): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "model1 = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, train_dataset.vocab.__len__(), train_dataset.tags.__len__(), vocab=train_dataset.vocab)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model1.parameters(), lr=0.1)\n",
    "# for p in model.parameters():\n",
    "#     if p.requires_grad == True:\n",
    "#         print(p.shape)\n",
    "# print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "train_dataset = ELL881Dataset(data_dir=\"E:\\IIT DELHI\\SEMESTER-8\\ELL881\\ASSIGNMENTS\\A2\\data\")\n",
    "test_dataset = ELL881Dataset(data_dir=\"E:\\IIT DELHI\\SEMESTER-8\\ELL881\\ASSIGNMENTS\\A2\\data\", split=\"test\")\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, train_dataset.vocab.__len__(), train_dataset.tags.__len__(), vocab=train_dataset.vocab)\n",
    "trainer = Trainer(model=model1, train_dataset=train_dataset, eval_dataset=test_dataset, optimizer=optimizer)\n",
    "trainer.train()\n",
    "\n",
    "torch.save(model.state_dict(), \"E:\\IIT DELHI\\SEMESTER-8\\ELL881\\ASSIGNMENTS\\A2\\models\\pytorch_model.bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model1 = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, train_dataset.vocab.__len__(), train_dataset.tags.__len__(), vocab=train_dataset.vocab)\n",
    "# model1.load_state_dict(torch.load(\"E:\\IIT DELHI\\SEMESTER-8\\ELL881\\ASSIGNMENTS\\A2\\models\\pytorch_model.bin\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-NP', 'B-PP', 'B-NP', 'I-NP', 'B-VP', 'I-VP', 'I-VP', 'I-VP', 'I-VP', 'B-NP']\n",
      "['B-NP', 'B-PP', 'B-NP', 'I-NP', 'B-VP', 'I-VP', 'I-VP', 'I-VP', 'I-VP', 'B-NP']\n",
      "Micro F1 score:  0.9585584115327077\n",
      "Macro F1 score:  0.5339874427631582\n",
      "Average F1 score:  0.7462729271479329\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# trainer = Trainer(model=model1, train_dataset=train_dataset, eval_dataset=test_dataset, optimizer=optimizer)\n",
    "predY = trainer.predict(train_dataset)\n",
    "\n",
    "print(predY[0:10])\n",
    "\n",
    "trueY = []\n",
    "for i in range(len(train_dataset)):\n",
    "    for tag in train_dataset.Y[i]:\n",
    "        trueY.append(tag)\n",
    "print(trueY[0:10])\n",
    "trainer.evaluate(trueY, predY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
